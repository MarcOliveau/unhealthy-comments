{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online. Eeach comment is either labeled healthy or unhealthy, and this is done through the use of 7 sub attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation (7) generalisation. These sub attributes allow the model to distinguish if a comment is healthy or not by both seeing if the sub-attribute is True (1) or False (0), and by using the confidence score of the sub-attributes. \n",
    "\n",
    "This notebook will walk you thorugh how ensemble learning can be used to predict if a comment will be either healthy or unhealthy. \n",
    "\n",
    "The data we will be using comes form two csv files provided by UCC, those being train.csv and test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First we need to import the 3 most important libraries in python machine learning. Also, we want to convert the train dataset into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>comment</th>\n",
       "      <th>antagonise</th>\n",
       "      <th>antagonise:confidence</th>\n",
       "      <th>condescending</th>\n",
       "      <th>condescending:confidence</th>\n",
       "      <th>dismissive</th>\n",
       "      <th>dismissive:confidence</th>\n",
       "      <th>generalisation</th>\n",
       "      <th>generalisation:confidence</th>\n",
       "      <th>generalisation_unfair</th>\n",
       "      <th>generalisation_unfair:confidence</th>\n",
       "      <th>healthy</th>\n",
       "      <th>healthy:confidence</th>\n",
       "      <th>hostile</th>\n",
       "      <th>hostile:confidence</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>sarcastic:confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1739460326</td>\n",
       "      <td>5</td>\n",
       "      <td>proving there is no cure for stupidity.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2297540155</td>\n",
       "      <td>5</td>\n",
       "      <td>Personally I prefer the Flying Spaghetti Monst...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1812168131</td>\n",
       "      <td>5</td>\n",
       "      <td>Your comparing a pipeline to a well? One that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8063</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1739470334</td>\n",
       "      <td>5</td>\n",
       "      <td>who is writing this pap!?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5959</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1739466190</td>\n",
       "      <td>3</td>\n",
       "      <td>Natives refuse to even consider that their cur...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id  _trusted_judgments  \\\n",
       "0  1739460326                   5   \n",
       "1  2297540155                   5   \n",
       "2  1812168131                   5   \n",
       "3  1739470334                   5   \n",
       "4  1739466190                   3   \n",
       "\n",
       "                                             comment  antagonise  \\\n",
       "0            proving there is no cure for stupidity.           1   \n",
       "1  Personally I prefer the Flying Spaghetti Monst...           0   \n",
       "2  Your comparing a pipeline to a well? One that ...           0   \n",
       "3                          who is writing this pap!?           0   \n",
       "4  Natives refuse to even consider that their cur...           0   \n",
       "\n",
       "   antagonise:confidence  condescending  condescending:confidence  dismissive  \\\n",
       "0                 0.5816              1                    0.5816           1   \n",
       "1                 1.0000              0                    1.0000           0   \n",
       "2                 1.0000              0                    0.8063           0   \n",
       "3                 0.7931              0                    0.5959           0   \n",
       "4                 1.0000              0                    1.0000           0   \n",
       "\n",
       "   dismissive:confidence  generalisation  generalisation:confidence  \\\n",
       "0                 0.5816               0                        1.0   \n",
       "1                 1.0000               0                        1.0   \n",
       "2                 1.0000               0                        1.0   \n",
       "3                 0.5959               0                        1.0   \n",
       "4                 1.0000               0                        1.0   \n",
       "\n",
       "   generalisation_unfair  generalisation_unfair:confidence  healthy  \\\n",
       "0                    0.0                               1.0        0   \n",
       "1                    0.0                               1.0        1   \n",
       "2                    0.0                               1.0        1   \n",
       "3                    0.0                               1.0        0   \n",
       "4                    0.0                               1.0        1   \n",
       "\n",
       "   healthy:confidence  hostile  hostile:confidence  sarcastic  \\\n",
       "0              0.5816      1.0              0.5816        0.0   \n",
       "1              0.7981      0.0              1.0000        0.0   \n",
       "2              0.6081      0.0              1.0000        0.0   \n",
       "3              0.7917      0.0              1.0000        0.0   \n",
       "4              1.0000      0.0              1.0000        0.0   \n",
       "\n",
       "   sarcastic:confidence  \n",
       "0                0.8001  \n",
       "1                1.0000  \n",
       "2                0.8063  \n",
       "3                0.6052  \n",
       "4                1.0000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What we want to check is that we have a balanced dataset, as if it is not balanced our data will be skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_healthy = dataset[dataset.healthy == 1]\n",
    "dataset_unhealthy = dataset[dataset.healthy == 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First lets check how many healthy comments there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        1\n",
       "2        1\n",
       "4        1\n",
       "5        1\n",
       "6        1\n",
       "        ..\n",
       "35498    1\n",
       "35499    1\n",
       "35500    1\n",
       "35501    1\n",
       "35502    1\n",
       "Name: healthy, Length: 32848, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_healthy[\"healthy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now lets look at how many unhealthy comments there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "3        0\n",
       "17       0\n",
       "29       0\n",
       "41       0\n",
       "        ..\n",
       "35430    0\n",
       "35458    0\n",
       "35466    0\n",
       "35490    0\n",
       "35497    0\n",
       "Name: healthy, Length: 2655, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unhealthy[\"healthy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There is clearly an imbalance between unhealhty and healhty comments, which would give us skewed results. If we were to use this data we could have the model predict 1 for any comment and have an accuracy score of above 90%. What we want to do is balance the dataset by upsampling the minority (unhealhty comments) and downsampling tha majority (healhty comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "dataset_unhealthy_upsampled = resample(dataset_unhealthy,\n",
    "                                      replace=True,\n",
    "                                      n_samples=20000,\n",
    "                                      random_state=123)\n",
    "dataset_healthy_downsampled = resample(dataset_healthy,\n",
    "                                        replace=False,\n",
    "                                        n_samples=20000,\n",
    "                                        random_state=123)\n",
    "train_dataset = pd.concat([dataset_healthy_downsampled, dataset_unhealthy_upsampled])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now let us check how balanced the new dataset is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27173    1\n",
      "32171    1\n",
      "32036    1\n",
      "17321    1\n",
      "26325    1\n",
      "        ..\n",
      "26798    1\n",
      "32108    1\n",
      "7360     1\n",
      "8128     1\n",
      "1054     1\n",
      "Name: healthy, Length: 20000, dtype: int64\n",
      "18151    0\n",
      "14849    0\n",
      "23955    0\n",
      "29069    0\n",
      "15192    0\n",
      "        ..\n",
      "13998    0\n",
      "30137    0\n",
      "1370     0\n",
      "19554    0\n",
      "25081    0\n",
      "Name: healthy, Length: 20000, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "new_dataset_healthy = train_dataset[dataset.healthy == 1]\n",
    "print(new_dataset_healthy[\"healthy\"])\n",
    "new_dataset_unhealthy = train_dataset[dataset.healthy == 0]\n",
    "print(new_dataset_unhealthy[\"healthy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Now that the data is balanced, we cna move on the select features form the data that are useful from the train_dataset. The columns we will be selecting are all the sub-attributes and their confidence scores for the X_data and the \"healthy\" column as the Y_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set_1 = train_dataset.iloc[:, 3:13].values\n",
    "df_X_1 = pd.DataFrame(X_set_1,\n",
    "                      columns=[\"antagonise\", \"antagonise:confidence\", \"condescending\", \"condescending:confidence\",\n",
    "                               \"dismissive\", \"dismissive:confidence\", \"generalisation\", \"generalisation:confidence\",\n",
    "                               \"generalisation_unfair\", \"generalisation_unfair:confidence\"])\n",
    "X_set_2 = train_dataset.iloc[:, 15:].values\n",
    "df_X_2 = pd.DataFrame(X_set_2, columns=[\"hostile\", \"hostile:confidence\", \"sarcastic\", \"sarcastic:confidence\"])\n",
    "\n",
    "X_data_train = pd.concat([df_X_1.reset_index(drop=True),\n",
    "                          df_X_2.reset_index(drop=True)],\n",
    "                         axis=1,\n",
    "                         ignore_index=True)\n",
    "X_data_columns = [\n",
    "    list(df_X_1.columns),\n",
    "    list(df_X_2.columns)]\n",
    "\n",
    "flatten = lambda nested_lists: [item for sublist in nested_lists for item in sublist]\n",
    "X_data_train.columns = flatten(X_data_columns)\n",
    "\n",
    "\n",
    "y_data = train_dataset.iloc[:, 13].values\n",
    "y_train = pd.DataFrame(y_data, columns=[\"Healthy\"])\n",
    "y_train = y_train.values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antagonise</th>\n",
       "      <th>antagonise:confidence</th>\n",
       "      <th>condescending</th>\n",
       "      <th>condescending:confidence</th>\n",
       "      <th>dismissive</th>\n",
       "      <th>dismissive:confidence</th>\n",
       "      <th>generalisation</th>\n",
       "      <th>generalisation:confidence</th>\n",
       "      <th>generalisation_unfair</th>\n",
       "      <th>generalisation_unfair:confidence</th>\n",
       "      <th>hostile</th>\n",
       "      <th>hostile:confidence</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>sarcastic:confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antagonise  antagonise:confidence  condescending  condescending:confidence  \\\n",
       "0         0.0                 1.0000            0.0                    1.0000   \n",
       "1         0.0                 1.0000            0.0                    1.0000   \n",
       "2         0.0                 0.6134            0.0                    0.6134   \n",
       "3         0.0                 1.0000            0.0                    1.0000   \n",
       "4         0.0                 1.0000            0.0                    1.0000   \n",
       "\n",
       "   dismissive  dismissive:confidence  generalisation  \\\n",
       "0         0.0                 1.0000             0.0   \n",
       "1         0.0                 1.0000             0.0   \n",
       "2         0.0                 0.6134             0.0   \n",
       "3         0.0                 1.0000             0.0   \n",
       "4         0.0                 1.0000             0.0   \n",
       "\n",
       "   generalisation:confidence  generalisation_unfair  \\\n",
       "0                        1.0                    0.0   \n",
       "1                        1.0                    0.0   \n",
       "2                        1.0                    0.0   \n",
       "3                        1.0                    0.0   \n",
       "4                        1.0                    0.0   \n",
       "\n",
       "   generalisation_unfair:confidence  hostile  hostile:confidence  sarcastic  \\\n",
       "0                               1.0      0.0              1.0000        0.0   \n",
       "1                               1.0      0.0              1.0000        0.0   \n",
       "2                               1.0      0.0              0.8026        0.0   \n",
       "3                               1.0      0.0              0.8046        0.0   \n",
       "4                               1.0      0.0              1.0000        0.0   \n",
       "\n",
       "   sarcastic:confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8026  \n",
       "3                0.8046  \n",
       "4                1.0000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If we observe the X_data, we can see that the confidence scores of the attributes are relative to the boolean value of the attribute. What we want to do is have the confidence score be the same type (either all unhealhty confidence score or all healthy scores), as then the model can make better sense of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = X_data_train[\"antagonise\"] - X_data_train[\"antagonise:confidence\"]\n",
    "c2 = X_data_train[\"condescending\"] - X_data_train[\"condescending:confidence\"]\n",
    "c3 = X_data_train[\"dismissive\"] - X_data_train[\"dismissive:confidence\"]\n",
    "c4 = X_data_train[\"generalisation\"] - X_data_train[\"generalisation:confidence\"]\n",
    "c5 = X_data_train[\"generalisation_unfair\"] - X_data_train[\"generalisation_unfair:confidence\"]\n",
    "c6 = X_data_train[\"hostile\"] - X_data_train[\"hostile:confidence\"]\n",
    "c7 = X_data_train[\"sarcastic\"] - X_data_train[\"sarcastic:confidence\"]\n",
    "\n",
    "X_train = pd.DataFrame({\"antagonise\": c1,\n",
    "                        \"condescending\": c2,\n",
    "                        \"dismissive\": c3,\n",
    "                        \"generalisation\": c4,\n",
    "                        \"generalisation_unfair\": c5,\n",
    "                        \"hostile\": c6,\n",
    "                        \"sarcastic\": c7},\n",
    "                       )\n",
    "X_train = X_train.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antagonise</th>\n",
       "      <th>condescending</th>\n",
       "      <th>dismissive</th>\n",
       "      <th>generalisation</th>\n",
       "      <th>generalisation_unfair</th>\n",
       "      <th>hostile</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.8026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8046</td>\n",
       "      <td>0.8046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antagonise  condescending  dismissive  generalisation  \\\n",
       "0      1.0000         1.0000      1.0000             1.0   \n",
       "1      1.0000         1.0000      1.0000             1.0   \n",
       "2      0.6134         0.6134      0.6134             1.0   \n",
       "3      1.0000         1.0000      1.0000             1.0   \n",
       "4      1.0000         1.0000      1.0000             1.0   \n",
       "\n",
       "   generalisation_unfair  hostile  sarcastic  \n",
       "0                    1.0   1.0000     1.0000  \n",
       "1                    1.0   1.0000     1.0000  \n",
       "2                    1.0   0.8026     0.8026  \n",
       "3                    1.0   0.8046     0.8046  \n",
       "4                    1.0   1.0000     1.0000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we have all the confidence scores relative to unhealthy attributes, 1 being 100 percent confident the ocmment is unhealhty and 0 being 100 percent confidence the commment is healhty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The whole process is now repeated with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('test.csv')\n",
    "X_set_1 = test_dataset.iloc[:, 3:13].values\n",
    "df_X_1 = pd.DataFrame(X_set_1,\n",
    "                      columns=[\"antagonise\", \"antagonise:confidence\", \"condescending\", \"condescending:confidence\",\n",
    "                               \"dismissive\", \"dismissive:confidence\", \"generalisation\", \"generalisation:confidence\",\n",
    "                               \"generalisation_unfair\", \"generalisation_unfair:confidence\"])\n",
    "X_set_2 = test_dataset.iloc[:, 15:].values\n",
    "df_X_2 = pd.DataFrame(X_set_2, columns=[\"hostile\", \"hostile:confidence\", \"sarcastic\", \"sarcastic:confidence\"])\n",
    "\n",
    "X_data_test = pd.concat([df_X_1.reset_index(drop=True),\n",
    "                         df_X_2.reset_index(drop=True)],\n",
    "                        axis=1,\n",
    "                        ignore_index=True)\n",
    "X_data_columns = [\n",
    "    list(df_X_1.columns),\n",
    "    list(df_X_2.columns)]\n",
    "\n",
    "flatten = lambda nested_lists: [item for sublist in nested_lists for item in sublist]\n",
    "X_data_test.columns = flatten(X_data_columns)\n",
    "\n",
    "c1 = X_data_test[\"antagonise\"] - X_data_test[\"antagonise:confidence\"]\n",
    "c2 = X_data_test[\"condescending\"] - X_data_test[\"condescending:confidence\"]\n",
    "c3 = X_data_test[\"dismissive\"] - X_data_test[\"dismissive:confidence\"]\n",
    "c4 = X_data_test[\"generalisation\"] - X_data_test[\"generalisation:confidence\"]\n",
    "c5 = X_data_test[\"generalisation_unfair\"] - X_data_test[\"generalisation_unfair:confidence\"]\n",
    "c6 = X_data_test[\"hostile\"] - X_data_test[\"hostile:confidence\"]\n",
    "c7 = X_data_test[\"sarcastic\"] - X_data_test[\"sarcastic:confidence\"]\n",
    "\n",
    "X_test = pd.DataFrame({\"antagonise\": c1,\n",
    "                       \"condescending\": c2,\n",
    "                       \"dismissive\": c3,\n",
    "                       \"generalisation\": c4,\n",
    "                       \"generalisation_unfair\": c5,\n",
    "                       \"hostile\": c6,\n",
    "                       \"sarcastic\": c7},\n",
    "                      )\n",
    "X_test = X_test.abs()\n",
    "\n",
    "y_data = test_dataset.iloc[:, 13].values\n",
    "y_test = pd.DataFrame(y_data, columns=[\"Healthy\"])\n",
    "y_test.values.ravel()\n",
    "\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antagonise</th>\n",
       "      <th>condescending</th>\n",
       "      <th>dismissive</th>\n",
       "      <th>generalisation</th>\n",
       "      <th>generalisation_unfair</th>\n",
       "      <th>hostile</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7529</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antagonise  condescending  dismissive  generalisation  \\\n",
       "0      1.0000         1.0000      1.0000          1.0000   \n",
       "1      1.0000         1.0000      1.0000          0.8041   \n",
       "2      1.0000         1.0000      1.0000          1.0000   \n",
       "3      0.7529         0.7529      0.7529          0.7529   \n",
       "4      1.0000         1.0000      1.0000          1.0000   \n",
       "\n",
       "   generalisation_unfair  hostile  sarcastic  \n",
       "0                    1.0   1.0000     1.0000  \n",
       "1                    1.0   1.0000     1.0000  \n",
       "2                    1.0   1.0000     1.0000  \n",
       "3                    1.0   0.7529     1.0000  \n",
       "4                    1.0   1.0000     0.6068  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Since the Data has been preprocessed correctly, we can move on to the different ensemble methods we can use to make accurate models for prediction. We will be using three common techniques, those being bagging, boosting and stacking. We will be implementing them in the order stated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Random Forrest Classifier as a method of bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=15,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_leaf_nodes=15,\n",
    "    random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We produce a confusion matrix to get a more realistic roc auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 266   54]\n",
      " [ 653 3452]]\n",
      "accuracy score rf:  0.840225988700565 \n",
      "  roc auc score rf:  0.836087850182704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm)\n",
    "print(\"accuracy score rf: \",accuracy_score(y_test, y_pred_rf), \"\\n\"\n",
    "      \"  roc auc score rf: \", roc_auc_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cross Validation allows us to get the mean roc auc score and the standard deviation of those scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Accuracy: 91.74 %\n",
      "Standard Deviation: 0.40 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = rnd_clf,\n",
    "                             X = X_train, y = y_train,\n",
    "                             scoring= \"roc_auc\",\n",
    "                             cv = 10)\n",
    "print(\"ROC Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now lets use a boosting technique called Adaboosting in order to compare the results of this model with the Random Forrest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
       "                   n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost_clf = AdaBoostClassifier(n_estimators=100, learning_rate=1)\n",
    "boost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We produce a confusion matrix to get a more realistic roc auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 256   64]\n",
      " [ 555 3550]]\n",
      "accuracy score ab:  0.8601129943502824 \n",
      "  roc auc score ab:  0.8323995127892815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8601129943502824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ab = boost_clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_ab)\n",
    "print(cm)\n",
    "print(\"accuracy score ab: \",accuracy_score(y_test, y_pred_ab), \"\\n\"\n",
    "      \"  roc auc score ab: \", roc_auc_score(y_test, y_pred_ab))\n",
    "accuracy_score(y_test, y_pred_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cross Validation allows us to get the mean roc auc score and the standard deviation of those scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Accuracy: 91.71 %\n",
      "Standard Deviation: 0.38 %\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=boost_clf, X=X_train, y=y_train, scoring=\"roc_auc\", cv=10)\n",
    "print(\"ROC Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Important to note: we could have used another estimator for Adaboost than the default one (Decision Trees), such as Logistic Regression or K nearest neighbours. However, the default was the one that gave the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Another boosting technique we can use is one called XGBoosting technique, which whill be implemented below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.3.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.17.2)\n",
      "[[ 231   89]\n",
      " [ 366 3739]]\n",
      "accuracy score xg:  0.8971751412429378 \n",
      "  roc auc score xg:  0.8163577192448235\n",
      "ROC Accuracy xg: 95.20 %\n",
      "Standard Deviation xg: 0.35 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "## XGBoost Classifier\n",
    "xg_clf = XGBClassifier()\n",
    "xg_clf.fit(X_train, y_train)\n",
    "## Confsuion Matrix\n",
    "y_pred_xg = xg_clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_xg)\n",
    "print(cm)\n",
    "print(\"accuracy score xg: \",accuracy_score(y_test, y_pred_xg), \"\\n\"\n",
    "      \"  roc auc score xg: \", roc_auc_score(y_test, y_pred_xg))\n",
    "## Cross Validation\n",
    "accuracies_xg = cross_val_score(estimator = xg_clf, X=X_train, y=y_train, scoring= \"roc_auc\", cv=10)\n",
    "print(\"ROC Accuracy xg: {:.2f} %\".format(accuracies_xg.mean()*100))\n",
    "print(\"Standard Deviation xg: {:.2f} %\".format(accuracies_xg.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    XGBoost has a really solid roc auc score of 95%, making it the best model yet. Adaboosting gives us a score of 91.71%, and Random Forret gives us 91.74%. Another ensemble method we can use is Soft voting, as will be implemented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8349364342265531\n",
      "KNeighborsClassifier 0.8427527405602924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 0.8305648599269184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "svc_clf = SVC(probability=True)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"lr\", log_clf), (\"knn\", knn_clf), (\"svc\", svc_clf)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "for clf in (log_clf, knn_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The final ensemble method we will be using is the Stacking Classifier, using all the previous models used into one meta model (excluding Adaboost as the XGboost model performed better)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "stack_clf = StackingCVClassifier(classifiers=[voting_clf, rnd_clf, xg_clf],\n",
    "                                 shuffle=False,\n",
    "                                 use_probas=True,\n",
    "                                 cv=5,\n",
    "                                 meta_classifier=SVC(probability=True))\n",
    "classifiers = {\"voting\": voting_clf,\n",
    "               \"RF\": rnd_clf,\n",
    "               \"XG\": xg_clf,\n",
    "               \"Stack\": stack_clf}\n",
    "for key in classifiers:\n",
    "    classifier = classifiers[key]\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(key, roc_auc_score(y_pred, y_test))\n",
    "    classifiers[key] = classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
